# NumPy-Transformer-From-Scratch

* Pure NumPy: no torch, tensorflow, or autograd.
* Backpropagation: manual implementation of the backward pass for all layers.
* Modular Design: mimics the PyTorch nn.Module API.
* Optimizers: custom Adam.
* Inference: for computational reasons, simple reverse sequence task.

### Useful resources:

* Original Google Transformer paper: [Attention is All you Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
* Core concept of transformer explained: [Transformers from Scratch](https://brandonrohrer.com/transformers.html)
* Harvard pyTorch implementation: [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
* Widely usefull 3Blue1Brown explanation: [Neural networks](https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=YNa-KtC6UHkWgOrZ)

<img width="500" height="500" alt="image" src="https://github.com/user-attachments/assets/eded89e6-a065-4f27-9b8e-4c5653c53d45" />
